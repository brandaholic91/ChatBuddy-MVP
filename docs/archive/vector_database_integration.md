# VektoradatbÃ¡zis IntegrÃ¡ciÃ³ - Supabase pgvector

## ğŸ” ÃttekintÃ©s

A Chatbuddy MVP projektben Supabase pgvector extension-t hasznÃ¡ljuk vektoradatbÃ¡ziskÃ©nt a semantic search, RAG (Retrieval-Augmented Generation), Ã©s termÃ©kajÃ¡nlÃ¡si funkciÃ³k megvalÃ³sÃ­tÃ¡sÃ¡hoz.

---

## ğŸ¯ **MiÃ©rt VektoradatbÃ¡zis?**

### **HagyomÃ¡nyos vs. VektoralapÃº KeresÃ©s:**

| HagyomÃ¡nyos SQL | VektoralapÃº Semantic Search |
|-----------------|----------------------------|
| `WHERE title LIKE '%iPhone%'` | HasonlÃ³sÃ¡g-alapÃº keresÃ©s embeddingek kÃ¶zÃ¶tt |
| Pontos szÃ¶vegegyezÃ©s | JelentÃ©s-alapÃº keresÃ©s |
| SzinonimÃ¡kat nem ismeri fel | "telefon", "mobil", "smartphone" â†’ ugyanaz |
| NyelvfÃ¼ggÅ‘ | TÃ¶bbnyelvÅ± tÃ¡mogatÃ¡s |

### **HasznÃ¡lati Esetek:**
- **ğŸ” Semantic Search**: "Keresek egy jÃ³ telefont" â†’ iPhone, Samsung, Xiaomi eredmÃ©nyek
- **ğŸ§  RAG Implementation**: RelevÃ¡ns termÃ©kadatokkal kiegÃ©szÃ­tett LLM vÃ¡laszok
- **ğŸ¯ Recommendation Engine**: HasonlÃ³ termÃ©kek automatikus ajÃ¡nlÃ¡sa
- **ğŸ’¬ FAQ Matching**: FelhasznÃ¡lÃ³i kÃ©rdÃ©sek automatikus FAQ-hoz rendelÃ©se

---

## ğŸ—ï¸ **ArchitektÃºra**

### **Supabase pgvector Stack:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Query    â”‚ -> â”‚  OpenAI Embed   â”‚ -> â”‚  pgvector DB    â”‚
â”‚ "jÃ³ telefon"    â”‚    â”‚ text-embed-3-sm â”‚    â”‚ similarity <->  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Response    â”‚ <- â”‚ Pydantic AI     â”‚ <- â”‚ Top 5 Results   â”‚
â”‚ + Product Data  â”‚    â”‚ Agent + RAG     â”‚    â”‚ {name, price}   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Dependency Injection Pattern:**
```python
@dataclass
class ProductDependencies:
    webshop_api: Any
    vector_db: Any        # Supabase pgvector kapcsolat
    user_context: dict

@product_agent.tool
async def semantic_search(ctx: RunContext[ProductDependencies], query: str):
    embedding = await get_openai_embedding(query)
    return await ctx.deps.vector_db.similarity_search(embedding, limit=5)
```

---

## ğŸ› ï¸ **ImplementÃ¡ciÃ³s RÃ©szletek**

### **1. Database Schema (Supabase)**
```sql
-- Enable pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Products table with embeddings
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    title TEXT NOT NULL,
    description TEXT,
    price DECIMAL(10,2),
    category TEXT,
    embedding VECTOR(1536),  -- OpenAI text-embedding-3-small dimenziÃ³
    created_at TIMESTAMP DEFAULT NOW()
);

-- Vector similarity index
CREATE INDEX ON products USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- Semantic search function
CREATE OR REPLACE FUNCTION search_products(
    query_embedding VECTOR(1536),
    similarity_threshold FLOAT DEFAULT 0.5,
    match_count INT DEFAULT 10
)
RETURNS TABLE (
    id INT,
    title TEXT,
    description TEXT,
    price DECIMAL(10,2),
    similarity FLOAT
)
LANGUAGE SQL
AS $$
SELECT
    p.id,
    p.title,
    p.description,
    p.price,
    1 - (p.embedding <=> query_embedding) AS similarity
FROM products p
WHERE 1 - (p.embedding <=> query_embedding) > similarity_threshold
ORDER BY p.embedding <=> query_embedding
LIMIT match_count;
$$;
```

### **2. Python Integration**
```python
import openai
from supabase import create_client
import numpy as np

class VectorSearchService:
    def __init__(self, supabase_client, openai_client):
        self.supabase = supabase_client
        self.openai = openai_client
    
    async def get_embedding(self, text: str) -> List[float]:
        """OpenAI embedding generÃ¡lÃ¡sa"""
        response = await self.openai.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
    
    async def semantic_search(self, query: str, limit: int = 5) -> List[dict]:
        """Semantic search vÃ©grehajtÃ¡sa"""
        query_embedding = await self.get_embedding(query)
        
        result = self.supabase.rpc('search_products', {
            'query_embedding': query_embedding,
            'match_count': limit,
            'similarity_threshold': 0.5
        }).execute()
        
        return result.data
    
    async def batch_embed_products(self, products: List[dict]):
        """Batch embedding processing nagy adatbÃ¡zisokhoz"""
        batch_size = 100
        for i in range(0, len(products), batch_size):
            batch = products[i:i + batch_size]
            
            texts = [f"{p['title']} {p['description']}" for p in batch]
            embeddings = await self.get_embeddings_batch(texts)
            
            # Supabase batch update
            updates = [
                {"id": p["id"], "embedding": emb} 
                for p, emb in zip(batch, embeddings)
            ]
            
            self.supabase.table('products').upsert(updates).execute()
```

### **3. Pydantic AI Integration**
```python
from dataclasses import dataclass
from pydantic_ai import Agent, RunContext

@dataclass
class ChatbotDependencies:
    vector_search: VectorSearchService
    webshop_api: Any
    user_context: dict

product_agent = Agent(
    'openai:gpt-4o',
    deps_type=ChatbotDependencies,
    system_prompt="""
    Te egy termÃ©k szakÃ©rtÅ‘ vagy. HasznÃ¡lj semantic search-t 
    relevÃ¡ns termÃ©kek megtalÃ¡lÃ¡sÃ¡hoz.
    """
)

@product_agent.tool
async def find_similar_products(
    ctx: RunContext[ChatbotDependencies], 
    query: str
) -> List[dict]:
    """HasonlÃ³ termÃ©kek keresÃ©se vektorkeresÃ©s alapjÃ¡n"""
    results = await ctx.deps.vector_search.semantic_search(query)
    return [
        {
            "title": r["title"],
            "price": f"{r['price']} Ft",
            "similarity": f"{r['similarity']:.2%}"
        }
        for r in results
    ]

@product_agent.tool
async def get_product_recommendations(
    ctx: RunContext[ChatbotDependencies], 
    product_id: int
) -> List[dict]:
    """TermÃ©kajÃ¡nlÃ¡sok egy adott termÃ©k alapjÃ¡n"""
    # Get the product's embedding
    product = ctx.deps.webshop_api.get_product(product_id)
    
    # Find similar products
    similar = await ctx.deps.vector_search.semantic_search(
        f"{product['title']} {product['description']}"
    )
    
    return [s for s in similar if s["id"] != product_id][:3]
```

---

## âš™ï¸ **KÃ¶rnyezeti VÃ¡ltozÃ³k**

Az `.env_example` fÃ¡jl frissÃ­tÃ©se:
```bash
# AI Provider Settings
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Vector Database Configuration (pgvector)
VECTOR_SIMILARITY_THRESHOLD=0.5
VECTOR_SEARCH_LIMIT=10
EMBEDDING_BATCH_SIZE=100

# Supabase (PostgreSQL + pgvector)
SUPABASE_URL=your_supabase_project_url
SUPABASE_SERVICE_KEY=your_supabase_service_role_key
```

---

## ğŸ“¦ **Dependencies FrissÃ­tÃ©se**

`requirements.txt` kiegÃ©szÃ­tÃ©s:
```bash
# Vector Database (pgvector)
pgvector>=0.2.5
sentence-transformers>=2.2.0  # Backup embedding option
numpy>=1.24.0
```

---

## ğŸ”„ **Migration Path**

### **1. Supabase Setup:**
```sql
-- 1. Enable pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- 2. Add embedding column to existing products table
ALTER TABLE products ADD COLUMN embedding VECTOR(1536);

-- 3. Create vector index
CREATE INDEX ON products USING ivfflat (embedding vector_cosine_ops);
```

### **2. Data Migration:**
```python
async def migrate_existing_products():
    """MeglÃ©vÅ‘ termÃ©kek embedding-jeinek generÃ¡lÃ¡sa"""
    products = supabase.table('products').select('*').is_('embedding', 'null').execute()
    
    vector_service = VectorSearchService(supabase, openai_client)
    await vector_service.batch_embed_products(products.data)
    
    print(f"Migrated {len(products.data)} products with embeddings")
```

### **3. Testing:**
```python
async def test_vector_search():
    """Vector search tesztelÃ©se"""
    vector_service = VectorSearchService(supabase, openai_client)
    
    results = await vector_service.semantic_search("okos telefon")
    
    for result in results:
        print(f"Title: {result['title']}")
        print(f"Similarity: {result['similarity']:.2%}")
        print("---")
```

---

## ğŸ“ˆ **Performance OptimalizÃ¡ciÃ³k**

### **1. Indexing Strategy:**
```sql
-- IVFFlat index nagy adatbÃ¡zisokhoz
CREATE INDEX ON products USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- HNSW index mÃ©g jobb performance-hez (pgvector 0.5.0+)
CREATE INDEX ON products USING hnsw (embedding vector_cosine_ops);
```

### **2. Query Optimization:**
```python
# Batch embedding generation
async def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
    response = await self.openai.embeddings.create(
        model="text-embedding-3-small",
        input=texts  # Batch processing
    )
    return [d.embedding for d in response.data]

# Connection pooling
from sqlalchemy.pool import QueuePool
engine = create_async_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=0
)
```

### **3. Caching Strategy:**
```python
from functools import lru_cache
import redis

class CachedVectorService(VectorSearchService):
    def __init__(self, supabase_client, openai_client, redis_client):
        super().__init__(supabase_client, openai_client)
        self.redis = redis_client
    
    async def get_embedding(self, text: str) -> List[float]:
        # Check cache first
        cache_key = f"embedding:{hash(text)}"
        cached = await self.redis.get(cache_key)
        
        if cached:
            return json.loads(cached)
        
        # Generate and cache
        embedding = await super().get_embedding(text)
        await self.redis.setex(cache_key, 3600, json.dumps(embedding))
        
        return embedding
```

---

## ğŸ¯ **KÃ¶vetkezÅ‘ LÃ©pÃ©sek**

1. **âœ… README Ã©s dokumentÃ¡ciÃ³ frissÃ­tve**
2. **âœ… Environment konfigurÃ¡ciÃ³k hozzÃ¡adva**  
3. **âœ… Requirements.txt frissÃ­tve**
4. **ğŸ”„ Supabase pgvector setup**
5. **ğŸ”„ Migration scripts implementÃ¡lÃ¡sa**
6. **ğŸ”„ Vector search service fejlesztÃ©se**
7. **ğŸ”„ Pydantic AI integration tesztelÃ©se**

---

## ğŸ“š **ReferenciÃ¡k**

- [Supabase pgvector Guide](https://supabase.com/docs/guides/database/extensions/pgvector)
- [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings)
- [pgvector GitHub](https://github.com/pgvector/pgvector)
- [Vector Similarity Search Best Practices](https://github.com/pgvector/pgvector#performance)